{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"JKK","text":"<p>The Vehicle Industry Research Center has been working since May 2011 in Gyor, Hungary at the Sz\u00e9chenyi University. Understanding and researching how people and vehicles cooperate is an essential skill when designing the future of traffic. We believe that fully self-driving (a.k.a. autonomous) technology can lead to safe, easy, and sustainable transportation. We are preparing for this new technology-to-come by studying and researching its fundamentals and exploring the possibilities it offers. This process helps us gain unique knowledge on the mixed field of mechatronics, robotics, and artificial intelligence. Future transportation can be safe, easy, and sustainable without compromises.</p> <p>One of our most researched topic is self-driving (a.k.a autonomous) vehicles. We believe that fully self-driving technology can lead to safe, easy and sustainable transportation. We are preparing for this new technology-to-come by studying and researching its fundamentals and exploring the possibilities it offers. This process helps us gain unique knowledge on the mixed field of mechatronics, robotics and artificial intelligence. Future transportation can be safe, easy and sustainable without compromises.</p> <p></p>"},{"location":"assets/lab/","title":"Laboratory","text":"<p>The Vehicle Industry Research Center operates multiple laboratories, both at the ZalaZONE facility and on our Gy\u0151r campus.</p>"},{"location":"assets/lab/#battery-and-electronics-test-laboratory","title":"Battery and Electronics Test Laboratory","text":"<p>Location: Gy\u0151r, Capmus (Hungary)</p>"},{"location":"assets/lab/#climate-controlled-battery-testing-environment","title":"Climate-Controlled Battery Testing Environment","text":"<p>Our laboratory is equipped with a high-performance dynamic climate chamber (Binder LIT-MK 720), enabling precise thermal control for battery measurements under varying environmental conditions. The chamber offers a wide temperature range from -40C\u00b0 to +110C\u00b0, allowing for comprehensive thermal profiling of cells and packs. For enhanced operational safety during battery testing, the system features the P Plus safety package, suitable for tests up to EUCAR hazard level 5. Additional safety measures include an inert gas connection and a fully automatic CO\u2082 fire extinguishing system, making the chamber ideal for both development and safety validation of lithium-ion battery systems. </p>"},{"location":"assets/lab/#battery-measurement-and-emulation-systems","title":"Battery Measurement and Emulation Systems","text":"<p>The laboratory features an advanced NHR9200 modular measurement and emulation system designed for high precision battery testing. This bidirectional power system supports power levels from up to 24 kW and operates  within a voltage range of 5\u2013120 V. The system includes a regenerative discharge option for energy-efficient operation. It is integrated within a National Instruments-based measurement environment, enabling complex test automation and real-time data acquisition. The setup supports a wide range of test applications including cyclic measurements, performance benchmarking, and in-depth battery characterization, making it a versatile tool for both research and industrial applications.  </p>"},{"location":"assets/lab/#additional-infrastructure-and-safety-systems","title":"Additional Infrastructure and Safety Systems","text":"<p>Complementing the core systems, our lab includes multiple 5 kW EA power supplies and electronic loads, a Gamry electrochemical impedance spectroscopy (EIS) measurement unit, and a custom NI CompactRIO-based system. This measurement system enables up to 16 channels of active cell balancing and 24 channels of cell voltage and temperature monitoring, supporting both cell-level and pack level measurements up to 60 V and 200 A. For storage and charging safety, we utilize D\u00fcperthal Battery Standard XL safety cabinets, which offer 90 minutes of fire resistance and comply with DIN EN 14470-1 and DIN EN 14727. These cabinets feature automatic fire responsive door closures, fire escape locks, and exhaust connections. Additionally, for safe battery handling and transport, the lab is equipped with a fire extinguisher system specifically designed for lithium battery applications.</p>"},{"location":"assets/lab/#electric-powertrain-testbench","title":"Electric Powertrain Testbench","text":"<p>Location: Gy\u0151r, Campus (Hungary)</p> <p>In the Electric Powertrain Testbench Laboratory is able to test electrical machines on different power levels. The high-performance testbench can currently be tested with a measuring range of 250 Nm and 4000 rpm (by brake machine, but this range can be modified with a gearbox). On the low-power testbench can perform drive tests up to 5 kW in the range of 100 Nm and 800 rpm. It is also possible to perform other task-dependent tests on the own developed testbench systems.</p> <p>Link: szolgaltatas.sze.hu/en_GB/electric-powertrain-testbench</p>"},{"location":"assets/lab/#calibration-and-diagnostics-laboratory","title":"Calibration and Diagnostics Laboratory","text":"<p>Location: ZalaZONE (Hungary)</p> <p>The basic function of the lab is to perform various vehicle-level sensor calibration and diagnostic tasks. In addition to basic measuring and diagnostic tools, the workshop is equipped with electronics and 3D printing workstations, which are essential for assembling the necessary measuring systems and manufacturing custom parts/prototypes. Autonomous test vehicles can be prepared here for test track measurements.</p>"},{"location":"assets/vehicles/","title":"Lexus RX450h","text":"<p>Sensors: 1x Ouster OS2-64 LIDAR, 2x OS1-32 LIDAR, 1x Luminar Iris, 1x Stereolabs Zed2i, 2x TIER IV C2, GPS(GNSS)/IMU, Fixposition Vision-RTK 2, 5x radars</p> Lexus"},{"location":"assets/vehicles/#nissan-leaf","title":"Nissan Leaf","text":"<p>Sensors: 2x Ouster OS1-64 LIDAR, 2x Velodyne VLP16 LIDAR, SICK LMS111 LIDAR, Stereolabs Zed / Zed2</p> Nissan Leaf"},{"location":"assets/vehicles/#traffic-cone-manipulation-robot","title":"Traffic Cone Manipulation Robot","text":"<p>Sensors: Intel RealSense RGB-D camera, IMU, GPS(GNSS)</p> Traffic Cone Manipulation Robot"},{"location":"assets/vehicles/#szenergy","title":"Szenergy","text":"<p>Sensors: Ouster OS1-128 LIDAR, SICK LMS111 LIDAR, Stereolabs Zed2i</p> Szenergy"},{"location":"assets/vehicles/#f110-ackermann-robot-roboworks-rosbot-mini-ackermann","title":"F1/10 (Ackermann robot) / Roboworks Rosbot mini Ackermann","text":"<p>Sensors: single channel LIDAR (laser scanner), RGB camera, IMU</p>"},{"location":"assets/zalazone/","title":"Test track","text":"<p>Our research center has direct access to ZalaZone test track, one of the most advanced automotive proving grounds in Europe. The track features a variety of road types and conditions, including urban environments, highways, and off-road sections, allowing us to conduct comprehensive testing of autonomous vehicles and advanced driver-assistance systems (ADAS).</p> ZalaZone test track ZalaZone test track ZalaZone test track"},{"location":"dataset/","title":"Dataset overview","text":""},{"location":"dataset/#jkk_dataset_01","title":"<code>JKK_DATASET_01</code>","text":"<p>This dataset consists of measurement log files (ROS 1 rosbag), pointcloud files and additional scripts to access and edit these. The data is provided for research and educational purposes.</p> <p>Details</p> <p></p>"},{"location":"dataset/#jkk_dataset_02","title":"<code>JKK_DATASET_02</code>","text":"<p>This dataset consists of measurement log files (ROS 2 mcap) with additional scripts to access and edit these. The data is provided for research and educational purposes.</p> <p>Details</p> <p></p>"},{"location":"dataset/#jkk_dataset_03","title":"<code>JKK_DATASET_03</code>","text":"<p>This dataset contains the Human-like Behavior (HLB) usecase data, in mat format.</p> <p>Details</p> <p></p>"},{"location":"dataset/jkk_dataset_01/","title":"jkk_dataset_01","text":"<p>The log data is in .bag format, the standard logging format for ROS. To simply view and play the data Foxglove Studio is the easiest solution. It works on Windows, Linux and Mac. Another popular option is MATLAB. The data can be imported, viewed and edited in MATLAB. If you are familiar with ROS C++ or python can be a good option too. Python also offers possibilities top open the rosbags without ROS, similarly to MATLAB. The postprocessed 3D pointcloud data is in .pcd (Point Cloud Data) file format, it is a common format used inside Point Cloud Library (PCL). Also pcd can be imported easily to MATLAB / python. One of our most researched topic is self-driving (a.k.a autonomous) vehicles. We believe that fully self-driving technology can lead to safe, easy and sustainable transportation. We are preparing for this new technology-to-come by studying and researching its fundamentals and exploring the possibilities it offers. This process helps us gain unique knowledge on the mixed field of mechatronics, robotics and artificial intelligence. Future transportation can be safe, easy and sustainable without compromises.</p>"},{"location":"dataset/jkk_dataset_01/#leaf-2022-03-18-gyorbag","title":"<code>leaf-2022-03-18-gyor.bag</code>","text":"<p>Size: <code>2.12 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EVlk6YgDtj9BrzIE8djt-rwBZ47q9NwcbgxU_zOuBji9IA?download=1 -O leaf-2022-03-18-gyor.bag\n</code></pre> <p></p>"},{"location":"dataset/jkk_dataset_01/#leaf-2021-04-23-campusbag","title":"<code>leaf-2021-04-23-campus.bag</code>","text":"<p>Size: <code>3.37 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EYl_ahy5pgBBhNHt5ZkiBikBoy_j_x95E96rDtTsxueB_A?download=1 -O leaf-2021-04-23-campus.bag\n</code></pre> <p></p>"},{"location":"dataset/jkk_dataset_01/#leaf-2021-07-02-zala-uni-trackbag","title":"<code>leaf-2021-07-02-zala-uni-track.bag</code>","text":"<p>Size: <code>1.16 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EaUlnq2KcQBHkCLB52nuPtQBw-FXYby23VUuwk6jmVzJBA?download=1 -O leaf-2021-07-02-zala-uni-track.bag\n</code></pre> <p></p>"},{"location":"dataset/jkk_dataset_01/#leaf-2020-06-10-campusbag","title":"<code>leaf-2020-06-10-campus.bag</code>","text":"<p>Size: <code>2.36 GB</code> </p> <p>Go to details</p> <p>Download from here or get it with <code>wget</code>:</p> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/ETGGWQ0z5FxDkj3vwsjRPJEBuMwnFavgEU9aF0ol4NlwDA?download=1 -O leaf-2020-06-10-campus.bag\n</code></pre>"},{"location":"dataset/jkk_dataset_02/","title":"jkk_dataset_02","text":"<p>The log data is in .mcap format, the standard logging format for <code>ROS 2</code>. <code>MCAP</code> is an open source container file format for multimodal log data. It supports multiple channels of timestamped pre-serialized data, and is ideal for use in pub/sub or robotics applications.</p>"},{"location":"dataset/jkk_dataset_02/#getting-started","title":"Getting started","text":""},{"location":"dataset/jkk_dataset_02/#download-the-mcap-bag-files","title":"Download the <code>mcap</code> (bag) files","text":"<p>Download every MCAP as a ZIP</p> <p>Download a sample MCAP</p> <p>You can instanly view the data in Foxglove Studio (Free, online or on ay platform).</p> <p></p>"},{"location":"dataset/jkk_dataset_02/#python-scripts-for-mcap","title":"Python scripts for MCAP","text":"<p>One of the easiest way to getting started with the dataset is to look at the examples:</p> <p>MCAP scripts examples</p> <p>Further MCAP in python notebook</p> <p>Important note</p> <p>MCAP is ROS 2 agnostic, and can be used in any Python project, but <code>rosbag2-api</code> does rely  on ROS2. Use <code>mcap-ros2-support</code> which is not dependent on ROS 2.</p> <p><code>Python MCAP ROS2 support</code> package (<code>mcap-ros2-support</code>) provides ROS2 support for the Python MCAP file format reader. It has no dependencies on ROS2 itself or a ROS 2 environment, and can be used in any Python project.</p> <p>On windows / linux / mac install the following packages:</p> <pre><code>pip install mcap mcap-ros2-support matplotlib numpy pandas scipy\n</code></pre>"},{"location":"dataset/jkk_dataset_02/#dataset-description","title":"Dataset description","text":"Route Name Description Terrain <code>nissan_zala_90_country_road_1</code> road section flat - no hills <code>nissan_zala_90_country_road_2</code> longer stretches of highway, slight bends, some roundabouts hilly road <code>nissan_zala_50_sagod</code> slightly winding roads with some sharper turns 1 slight uphill <code>nissan_zala_50_zeg_1</code> mostly going in one direction, interrupted by roundabouts, continuous going flat - no hills <code>nissan_zala_50_zeg_2</code> roundabouts, bends, stationary situations (due to traffic) flat - no hills <code>nissan_zala_50_zeg_3</code> square bends with parking flat - no hills <code>nissan_zala_50_zeg_4</code> winding flat - no hills <code>nissan_zala_90_mixed</code> dynamic, city and country road mostly flat, last about 10m hilly"},{"location":"dataset/jkk_dataset_02/#usage-in-ubuntu-windows-wsl","title":"Usage in Ubuntu / Windows WSL","text":""},{"location":"dataset/jkk_dataset_02/#install-mcap","title":"Install <code>mcap</code>:","text":"<pre><code>pip install mcap\n</code></pre>"},{"location":"dataset/jkk_dataset_02/#download-dataset-eg-to-mntcbagjkkds02","title":"Download dataset, e.g. to <code>/mnt/c/bag/jkkds02/</code>:","text":"<pre><code>cd /mnt/c/bag/jkkds02/\n</code></pre> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/EVofDCG_ORZJh--XTVLFsFEBOUYB1eAbHAzdTVDdf19Y9g?download=1 -O jkkds02.zip\n</code></pre> <p>Make sure you have <code>unzip</code> (<code>sudo apt-get install unzip</code>) and:</p> <pre><code>unzip jkkds02.zip\n</code></pre>"},{"location":"dataset/jkk_dataset_02/#some-images","title":"Some images","text":""},{"location":"dataset/jkk_dataset_03/","title":"jkk_dataset_03","text":"<p>This dataset contains the raw data of naturalistic driving, utilized for Human-Like Behavior studies of Automated Vehicles (HLB4AV).</p> <p></p> <p>The zipped measurements can be downloaded from the following link. Download from here.   To access the original files, along with the labelling traffic data, click here: Access data.</p> <p>The data contains information of 17 drivers, who were selected to have relevant driving experience. Dr001-Dr003 are professional drivers who have extra driving certificate. The following table shows the details of participants:</p> Driver ID Type Age Driving Experience Driving Frequency Milage per year 001 P 31 10+ 3 4 002 P 32 10+ 3 4 003 P 28 10+ 3 5 004 N-P 31 10+ 4 5 005 N-P 46 10+ 4 4 006 N-P 25 6-10 2 2 007 N-P 29 10+ 3 3 008 N-P 28 3-6 2 3 009 N-P 28 1-3 1 2 010 N-P 43 10+ 4 4 011 N-P 31 10+ 3 3 012 N-P 44 10+ 3 3 013 N-P 52 10+ 4 4 014 N-P 36 10+ 4 5 015 N-P 32 10+ 3 4 021 N-P 51 10+ 4 5 023 N-P 39 10+ 2 3 <p>Explanation to notations: - P: Professional, N-P: Non-Professional  - Driving Frequency: 1: few times a year, 2: few times a month, 3: few times a week, 4: every day. - Driving Milage per year: 1: 0-1000km, 2: 1000-3000km, 3: 5000-10000km, 4: 10000-25000km, 5: more than 25000km.</p> <p>The dataset uses the following coordinate system of the vehicle:</p> <p></p> <p>Even though the data is recorded considering only two dimensional movement, the Z axis is displayed to give the right explanation of the yaw rate signal. Always, positive direction of a rotational quantity means CCW direction.</p>"},{"location":"dataset/jkk_dataset_03/#reference-platform","title":"Reference Platform","text":"<p>Data was recorded using two different vehicle platforms.</p> <p>The reference platform is a dedicated test vehicle, equipped with multiple environment sensors, also with direct access to the CAN network of the vehicle. The type of the vehicle is a Volkswagen Golf VII Variant, with a 1.4 TSI engine and 7-shift automatic gearbox.</p> <p>There are various sensor devices in this vehicle: - Genesys ADMA Gen3 (DGPS), source: ADMA 3.0 Technical Documentation, Document revision: 1.9, Date: 02/2019. Device includes the sensing of vehicle kinematic states. - Bosch Second Generation Multi-Purpose Camera (MPC2.5), including lane edge detection. - Bosch Fourth Generation Radar sensor, including object detection in ego lane and the two neighbouring lanes. - CAN data, steering torque and steering angle values.</p> <p>Reference Platform was used for measurements with Driver 1, 2 and 3 (professional drivers).</p>"},{"location":"dataset/jkk_dataset_03/#test-platform","title":"Test Platform","text":"<p>The test platform is a vehicle which is used when nonprofessional drivers are measured. The test vehicle is a Skoda Octavia MK3, with automatic gearbox. The vehicle systems were not modified, and no driver-assistance function was activated during testing. This vehicle was only equipped with the Genesys ADMA Gen3 (DGPS) device. The lane position information was reconstructed from the static lane map and the localization of the vehicle.</p>"},{"location":"dataset/jkk_dataset_03/#offline-calculations","title":"Offline Calculations","text":""},{"location":"dataset/jkk_dataset_03/#static-lane-map","title":"Static Lane Map","text":"<p>Based on the reference measurements, the lane map of the test route was created. For this, the lane position provided by the video camera was used. The lane position accuracy has +/- 2cm, while the localization has accuracy of +/- 1 cm.</p> <p></p> <p>The camera provides information of the position of the lane edges at each sample time. The following information are provided: - lane position ($d$) - lane orientation ($\\Theta$) - lane curvature ($\\kappa$). These data are then interpolated to a resolution of 5 cm travel distance through the route, using MATLAB function spline. Also, the lane edge position are transformed to the global UTM coordinates considering the pose of the vehicle. In the end, the map data contains high resolution geometry of the lane edges and also the higher level geometrical quantities (orientation and curvature).</p>"},{"location":"dataset/jkk_dataset_03/#lane-reconstruction","title":"Lane Reconstruction","text":"<p>For the test platform, the static lane map information is transferred back to the vehicle coordinate frame at each time sample. This replaced the video camera information, therefore the lane position of the test vehicle is available, with an accuracy of +/- 3 cm. Unfortunately, the dynamic information (e.g., other objects in the lane) are not available for the test platform.</p>"},{"location":"dataset/jkk_dataset_03/#dash-cam-data","title":"Dash-cam data","text":"<p>For multiple drivers' data (currently noted by \"_withTraffic\" tag in the file name) a dash cam video is available. This video is not applicable to use for e.g., computer vision algorithms, but serve more as an informal source of traffic data. However, by manual labelling the following information were added to the data:  1. Oncoming traffic time to pass: when a vehicle appears on the camera image, a timer starts to count down from a certain initial value, until the vehicle passes the test vehicle. Initial value is 2.38 seconds for small vehicles (higher preceeding speed) and 3.38 seconds for trucks (lower preceeding speed). Times were calculated based on the experiments. When there is no oncoming traffic detected, time-to-pass is set to 2.38 seconds, reflecting the fact that a vehicle may turn up in any minute. Therefore, drivers are assumed to be prepared as there would already be oncoming traffic. When a vehicle is followed, the initial value is decreased to the half for both types of vehicles. 2. Oncoming traffic type: 0: no oncoming traffic, 1: small vehicle, 2: truck and 3: convoy.</p> <p>An example of a convoy, passing the ego vehicle is shown here:</p> <p></p> <p>The same scenario on the dash cam video (snippets cut by hand):</p> <p></p> <p>Please be noted, that this information is added manually, therefore not suitable for quantitative evaluation, only qualitative!</p>"},{"location":"dataset/jkk_dataset_03/#summary-of-signals","title":"Summary of Signals","text":"Signal Name Description Unit Range Availability Source VelocityX Longitudinal velocity of the vehicle m/s 0 - 40 both platforms ADMA SteeringTorque Torque applied by the driver on the steering wheel Nm +/-3 reference platforms CAN LaneOrientation Orientation of the mid-lane in the ego frame (positive: CW, negative: CCW) rad +/- 0.1 both platforms Camera LaneEdgePositionRight Position of the lane edge of the ego lane on right side (positive: left, negative: right) m +/- 2 both platforms Camera LaneEdgePositionLeft Position of the lane edge of the ego lane on left side (positive: left, negative: right) m +/- 2 both platforms Camera LaneCurvature Curvature of the lane in the vehicle position 1/m +/- 0.005 both platforms Camera ObjectDistanceFront Distance of the vehicle in the ego lane (if no vehicle is present, value is zero) m 0-250 m reference platforms Radar ObjectVelocityFront Absolute velocity of the vehicle in the ego lane (if no vehicle is present, value is zero) m/s 0-40 reference platforms Radar ObjectAccelerationFront Absolute acceleration of the vehicle in the ego lane (if no vehicle is present, value is zero) m/s^2 +/- 10 reference platforms Radar YawRate Yawrate of the vehicle around the Z axis rad/s +/- 0.1 both platforms ADMA AccelerationX Longitudinal acceleration of the vehicle m/s^2 +/- 10 both platforms ADMA AccelerationY Longitudinal acceleration of the vehicle m/s^2 +/- 10 both platforms ADMA RoadWheelAngleFront Road-wheel-angle of the front wheels rad +/- 0.1 reference platforms CAN GPS_time Global GPS time ms - both platforms ADMA GPS_status Global GPS status enum 1: GPS, 2: RTK float, 4: RTK_course, 8: RTK_Fixed both platforms ADMA LongPos_abs Global longitudinal position of the vehicle \u00b0 - both platforms ADMA LatPos_abs Global lateral position of the vehicle \u00b0 - both platforms ADMA Relative_time Relative time stamp to the beginning of the measurement s - both platforms ADMA <p>The data is recorded in every 10 ms. Localization information is available at every 50 ms.</p>"},{"location":"dataset/jkk_dataset_03/#github-repo","title":"GitHub repo","text":"<p>The following repository contains algorithms for driver model analysis and prototypes for ADAS functions endowed with human-like features.</p> <p>github.com/gfigneczi1/hlb</p> <p>The following scripts are used for the data process:</p> <ul> <li>Lane reconstruction from reference data:     Run segmentor profile \"MapValidation\" and evaluator profile \"MapValidation\", based on this description: </li> </ul> <p>Evaluation description </p> <p>Place the raw mat files (without the map data) into the _temp folder.</p> <ul> <li>Traffic information for which dash-cam video is available:</li> </ul> <p>Traffic label procession </p> <p>In this case, you shall define the path of the corresponding xlsx file that contains the relevant traffic information.</p> <ul> <li>Standardize names for the proper storing:</li> </ul> <p>Standardize names </p> <ul> <li>Merge data for drivers where multiple short runs are available:</li> </ul> <p>Merge data </p> <p>For this script, you shall also store all unmerged data in the _temp folder.</p>"},{"location":"home/about/","title":"About us","text":"<ul> <li>Gy\u0151r, Hungary, Europe</li> <li>Sz\u00e9chenyi University IS Building 2nd floor</li> <li>+36 96 613 680</li> <li>jkk@sze.hu</li> <li>jkk.sze.hu</li> <li>youtube.com/jkk-sze-research</li> </ul>"},{"location":"home/social/","title":"Social media","text":"<ul> <li>youtube.com/jkk-sze-research</li> <li>instagram.com/jkk.sze</li> <li>youtube.com/szenergyteam</li> <li>instagram.com/szenergyteam</li> </ul>"},{"location":"home/videos/","title":"Videos","text":""},{"location":"papers/","title":"Selected papers with code","text":"Paper Repository and stars Ern\u0151 Horv\u00e1th, Claudiu Pozna, and Mikl\u00f3s Unger. Real-time lidar-based urban road and sidewalk detection for autonomous vehicles. Sensors, 2022. URL: https://github.com/jkk-research/urban_road_filter, doi:10.3390/s22010194. github.com/jkk-research/urban_road_filter Ern\u0151 Horv\u00e1th, Claudiu Pozna, P\u00e9ter K\u0151r\u00f6s, Csaba Hajdu, and \u00c1ron Ballagi. Theoretical background and application of multiple goal pursuit trajectory follower. Hungarian Journal of Industry and Chemistry, 48(1):11\u201317, 2020. github.com/jkk-research/wayp_plan_tools Gergo Ferenc Igneczi, Erno Horvath, Roland Toth, and Krisztian Nyilas. Curve trajectory model for human preferred path planning of automated vehicles. Automotive Innovation, pages 1\u201312, 2024. github.com/gfigneczi1/hlb Kriszti\u00e1n Enisz, Istv\u00e1n Szalay, and Ern\u0151 Horv\u00e1th. Localization robustness improvement for an autonomous race car using multiple extended kalman filters. Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering, 0(0):09544070241266281, 2024. doi:10.1177/09544070241266281. github.com/jkk-research/kalman_pos Norbert Mark\u00f3, Ern\u0151 Horv\u00e1th, Istv\u00e1n Szalay, and Kriszti\u00e1n Enisz. Deep learning-based approach for autonomous vehicle localization: application and experimental analysis. Machines, 2023. URL: https://www.mdpi.com/2075-1702/11/12/1079, doi:10.3390/machines11121079. github.com/jkk-research/pos-prediction Rahul Razdan, M. Ilhan Akbas, Suleiman O. Alsweiss, Angela Cheatham, Kenneth Ford, Ern\u0151 Horv\u00e1th, Laxima Niure Kandel, Mohammad Musa, Raivo Sell, and B\u00e1lint T\u00f3th. Product assurance in the age of artificial intelligence. Technical Report, SAE International, 2025. URL: https://saemobilus.sae.org/reports/product-assurance-age-artificial-intelligence-epr2025011, doi:10.4271/EPR2025011. Technical report, no code repository available"},{"location":"papers/#bibliography","title":"Bibliography","text":"<p>The list is generated from the <code>refs.bib</code> file.</p> <ol> <li> <p>Rahul Razdan, M. Ilhan Akbas, Suleiman O. Alsweiss, Angela Cheatham, Kenneth Ford, Ern\u0151 Horv\u00e1th, Laxima Niure Kandel, Mohammad Musa, Raivo Sell, and B\u00e1lint T\u00f3th. Product assurance in the age of artificial intelligence. Technical Report, SAE International, 2025. URL: https://saemobilus.sae.org/reports/product-assurance-age-artificial-intelligence-epr2025011, doi:10.4271/EPR2025011.\u00a0\u21a9</p> </li> <li> <p>Gerg\u0151 Ferenc Ign\u00e9czi, Ern\u0151 Horv\u00e1th, and Attila Borsos. Analysis of drivers\u2019 path follow behaviour. In ICINCO 2024, volume 2, 93 \u2013 100. 2024. URL: https://www.scopus.com/inward/record.uri?eid=2-s2.0-105002145066&amp;doi=10.5220%2f0012889100003822&amp;partnerID=40&amp;md5=b1da939e5a1612aea385e17d8be45ff1, doi:10.5220/0012889100003822.\u00a0\u21a9</p> </li> <li> <p>Ahmed Oultiligh, Hassan Ayad, Abdeljalil EL Kari, Mostafa Mjahed, Nada EL Gmili, Ern\u0151 Horv\u00e1th, and Claudiu Pozna. An improved ieho super-twisting sliding mode control algorithm for trajectory tracking of a mobile robot. Studies in Informatics and Control, 33(1):49 \u2013 60, 2024. URL: https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191085370&amp;doi=10.24846%2fv33i1y202405&amp;partnerID=40&amp;md5=1bd6620d6d1a80394b7dc80a59e9487b, doi:10.24846/v33i1y202405.\u00a0\u21a9</p> </li> <li> <p>Kriszti\u00e1n Enisz, Istv\u00e1n Szalay, and Ern\u0151 Horv\u00e1th. Localization robustness improvement for an autonomous race car using multiple extended kalman filters. Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering, 0(0):09544070241266281, 2024. doi:10.1177/09544070241266281.\u00a0\u21a9</p> </li> <li> <p>Mikl\u00f3s Unger, Ern\u0151 Horv\u00e1th, D\u00e1niel Pup, and Claudiu Radu Pozna. Towards robust lidar lane clustering for autonomous vehicle perception in ros 2. In 2024 IEEE International Conference on Mobility, Operations, Services and Technologies (MOST), volume, 229\u2013234. 2024. doi:10.1109/MOST60774.2024.00031.\u00a0\u21a9</p> </li> <li> <p>Gergo Ferenc Igneczi, Erno Horvath, Roland Toth, and Krisztian Nyilas. Curve trajectory model for human preferred path planning of automated vehicles. Automotive Innovation, pages 1\u201312, 2024.\u00a0\u21a9</p> </li> <li> <p>Gergo Ferenc Igneczi, Erno Horvath, and Krisztian Nyilas. A linear driver model of local path planning for lane driving. In 2023 IEEE 21st Jubilee International Symposium on Intelligent Systems and Informatics (SISY), volume, 000103\u2013000108. 2023. URL: https://ieeexplore.ieee.org/document/10417953, doi:10.1109/SISY60376.2023.10417953.\u00a0\u21a9</p> </li> <li> <p>Norbert Mark\u00f3, Ern\u0151 Horv\u00e1th, Istv\u00e1n Szalay, and Kriszti\u00e1n Enisz. Deep learning-based approach for autonomous vehicle localization: application and experimental analysis. Machines, 2023. URL: https://www.mdpi.com/2075-1702/11/12/1079, doi:10.3390/machines11121079.\u00a0\u21a9</p> </li> <li> <p>Rudolf Krecht, Tam\u00e1s Budai, Ern\u00f5 Horv\u00e1th, \u00c1kos Kov\u00e1cs, Nobert Mark\u00f3, and Mikl\u00f3s Unger. Network optimization aspects of autonomous vehicles: challenges and future directions. IEEE Network, 37(4):282\u2013288, 2023. URL: https://ieeexplore.ieee.org/document/10293243, doi:10.1109/MNET.007.2300023.\u00a0\u21a9</p> </li> <li> <p>Gergo Ferenc Igneczi and Erno Horvath. Node point optimization for local trajectory planners based on human preferences. In 2023 IEEE 21st World Symposium on Applied Machine Intelligence and Informatics (SAMI), volume, 000225\u2013000230. 2023. doi:10.1109/SAMI58000.2023.10044488.\u00a0\u21a9</p> </li> <li> <p>Ern\u0151 Horv\u00e1th, Claudiu Pozna, and Mikl\u00f3s Unger. Real-time lidar-based urban road and sidewalk detection for autonomous vehicles. Sensors, 2022. URL: https://github.com/jkk-research/urban_road_filter, doi:10.3390/s22010194.\u00a0\u21a9</p> </li> <li> <p>Gerg\u0151 Ign\u00e9czi and Ern\u0151 Horv\u00e1th. A clothoid-based local trajectory planner with extended kalman filter. In 2022 IEEE 20th Jubilee World Symposium on Applied Machine Intelligence and Informatics (SAMI), volume, 000467\u2013000472. 2022. doi:10.1109/SAMI54271.2022.9780857.\u00a0\u21a9</p> </li> <li> <p>Claudiu Pozna, Radu-Emil Precup, Ern\u0151 Horv\u00e1th, and Emil M. Petriu. Hybrid particle filter\u2013particle swarm optimization algorithm and application to fuzzy controlled servo systems. IEEE Transactions on Fuzzy Systems, 30(10):4286\u20134297, 2022. URL: https://ieeexplore.ieee.org/document/9697415, doi:10.1109/TFUZZ.2022.3146986.\u00a0\u21a9</p> </li> <li> <p>Gerg\u0151 Ign\u00e9czi, Ern\u0151 Horv\u00e1th, and D\u00e1niel Pup. Implementation of a self-developed model predictive control scheme for vehicle parking maneuvers. 2021. arXiv:2109.10075.\u00a0\u21a9</p> </li> <li> <p>Claudiu Radu Pozna, Csaba Antonya, and Ern\u00f6 Horv\u00e1th. Case study on the tactical level of an autonomous vehicle control. In 2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME), volume, 1\u20136. 2021. doi:10.1109/ICECCME52200.2021.9590868.\u00a0\u21a9</p> </li> <li> <p>Ern\u0151 Horv\u00e1th and Claudiu Radu Pozna. Clothoid-based trajectory following approach for self-driving vehicles. In 2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI), volume, 000251\u2013000254. 2021. doi:10.1109/SAMI50585.2021.9378664.\u00a0\u21a9</p> </li> <li> <p>Mikl\u00f3s Unger, Ern\u0151 Horv\u00e1th, and P\u00e9ter K\u0151r\u00f6s. Development of point-cloud processing algorithm for self-driving challenges. In 2020 IEEE 24th International Conference on Intelligent Engineering Systems (INES), volume, 91\u201396. 2020. doi:10.1109/INES49302.2020.9147201.\u00a0\u21a9</p> </li> <li> <p>P\u00e9ter K\u0151r\u00f6s, G\u00e1bor Szak\u00e1llas, P\u00e9ter Guly\u00e1s, Zolt\u00e1n Pusztai, Zolt\u00e1n Szeli, and Ern\u0151 Horv\u00e1th. Self-driving vehicle sensors from one-seated experimental to road-legal vehicle. In 2020 IEEE 24th International Conference on Intelligent Engineering Systems (INES), volume, 97\u2013102. 2020. doi:10.1109/INES49302.2020.9147181.\u00a0\u21a9</p> </li> <li> <p>Ern\u0151 Horv\u00e1th, Claudiu Pozna, P\u00e9ter K\u0151r\u00f6s, Csaba Hajdu, and \u00c1ron Ballagi. Theoretical background and application of multiple goal pursuit trajectory follower. Hungarian Journal of Industry and Chemistry, 48(1):11\u201317, 2020.\u00a0\u21a9</p> </li> <li> <p>Ern\u00f6 Horv\u00e1th, C. Pozna, and Radu-Emil Precup. Robot coverage path planning based on iterative structured orientation. Acta Polytechnica Hungarica, 15:231\u2013249, 01 2018. doi:10.12700/APH.15.1.2018.2.12.\u00a0\u21a9</p> </li> </ol>"},{"location":"smt25/","title":"Cancelled \u274c","text":"<p>Roboracer / F1tenth is cancelled in 2025, hopefully we can organize it in 2026! \u2714\ufe0f</p>"},{"location":"smt25/#about-f110-roboracer-autonomous-racing","title":"About F1/10 (Roboracer) Autonomous Racing","text":"<p>F1TENTH (F1/10) Autonomous Racing is a semi-regular competition organized by an international community of researchers, engineers, and autonomous systems enthusiasts. The teams participating in an F1TENTH compatition will build a 1:10 scaled autonomous race car according to a given specification and write software for it to fulfill the objectives for the competition: Don\u2019t crash and minimize laptime.</p> F1TENTH car"},{"location":"smt25/#about-smt-2026","title":"About SMT 2026","text":"<p>The 3rd Sustainable Mobility and Transportation Symposium, an interdisciplinary scientific conference organized by the Audi Hungaria Faculty of Vehicle Engineering and the Vehicle Industry Reserach Center of Sz\u00e9chenyi Istv\u00e1n University. The conference is dedicated to exploring the latest advancements and innovations in the field of automotive technology and transportation, and aims to bring together experts from various domains to foster collaboration, share insights, and shape the future of mobility. </p>"},{"location":"smt25/#official-websites","title":"Official Websites","text":"<ul> <li>smtsymposium.sze.hu</li> <li>roboracer.ai</li> </ul>"},{"location":"smt25/#place","title":"Place","text":"<p>Europe, Hungary, Gy\u0151r, Sz\u00e9chenyi Istv\u00e1n University</p>"},{"location":"smt25/#race-resources","title":"Race Resources","text":"<p>All necessary information about the race e.g. rules, scoring system, simulation environments and track data will be displayed here.</p> <ul> <li>Rules</li> <li>Frequently Asked Questions</li> <li>In-person competition</li> </ul>"},{"location":"smt25/#flyer","title":"Flyer","text":"<p>View flyer</p>"},{"location":"workshops/","title":"Workshops overview","text":""},{"location":"workshops/#20251105-gyor-ros-2-wheeltec-roboracer","title":"<code>2025.11.05</code> Gy\u0151r <code>ROS 2</code>: Wheeltec &amp; Roboracer","text":"<p>During the workshop Wheeltec and Roboracer based real-world challage was carried out, based on <code>ROS 2</code> Humble.</p> <p></p>"},{"location":"workshops/#20240608-gyor-ros-2","title":"<code>2024.06.08</code> Gy\u0151r <code>ROS 2</code>","text":"<p>During the workshop F1/10 based simulation was presented based on <code>ROS 2</code> Humble.</p> <p> </p>"},{"location":"workshops/#20240418-kempten-ros-2","title":"<code>2024.04.18</code> Kempten <code>ROS 2</code>","text":"<p>During the workshop F1/10 hands-on session was presented based on <code>ROS 2</code> Humble.</p> <p> </p>"},{"location":"workshops/#20231103-gyor-ros-1","title":"<code>2023.11.03</code> Gy\u0151r <code>ROS 1</code>","text":"<p>During the workshop a very simple wall following approach was presented with hands-on experinece on a real robot. </p> <p> </p>"},{"location":"workshops/clustering_a/","title":"Workshop on ROS 2 LIDAR clustering","text":"<p>This short workshop will guide you through filtering LIDAR data into objects. Objects could be pedestrians, cars, buildings, and so on. This workshop is ROS 2 compatible. </p>"},{"location":"workshops/clustering_a/#requirements-high-level-overview","title":"Requirements (high-level overview)","text":"<ol> <li>ROS 2 Humble: \ud83d\udfe0 see previous workshops or docs.ros.org/en/humble/Installation.html </li> <li>A log file with raw LIDAR data (MCAP format) \u2705 </li> <li>The <code>patchworkpp</code> package to filter out the ground plane \u2705 </li> <li>The <code>lidar_cluster</code> package to perform the clustering \u2705 </li> </ol>"},{"location":"workshops/clustering_a/#video-overview","title":"Video overview","text":"<p>The following screen recording demonstrates the steps involved:</p>"},{"location":"workshops/clustering_a/#step-1-download-the-raw-data","title":"<code>Step 1.</code> - Download the raw data","text":"<p>In order to cluster LIDAR data, first you need - no surprise - LIDAR data. Use any of the following 3 options.</p>"},{"location":"workshops/clustering_a/#option-a-download-our-mcap-from-the-link-below","title":"<code>Option A.</code> - Download our MCAP from the link below","text":"<p>Download MCAP [~540MB]  </p> <p>In our examples the <code>.mcap</code> file is going to be saved in <code>/mnt/c/bag/</code>. If you want to use another directory, please change it accordingly.</p>"},{"location":"workshops/clustering_a/#option-b-download-our-mcap-through-your-terminal","title":"<code>Option B.</code> - Download our MCAP through your terminal","text":"Don't forget to change directory first.  In our case `/mnt/c/bag/` is used as a final destination:  <pre><code>cd /mnt/c/bag/\n</code></pre> <pre><code>wget https://laesze-my.sharepoint.com/:u:/g/personal/herno_o365_sze_hu/Eclwzn42FS9GunGay5LPq-EBA6U1dZseBFNDrr6P0MwB2w?download=1  -O lexus3-2024-04-05-gyor.mcap\n</code></pre>"},{"location":"workshops/clustering_a/#option-c-use-your-own-mcap","title":"<code>Option C.</code> - Use your own MCAP","text":"<p>You can use your own MCAP, but in that case, you may need to change following:</p> <ul> <li>The LIDAR topic <ul> <li>In our examples it's <code>/lexus3/os_center/points</code></li> </ul> </li> <li>LIDAR frame <ul> <li>In our examples it's <code>lexus3/os_center_a_laser_data_frame</code></li> </ul> </li> </ul> <p>Never forget to update these in later steps if you use your own MCAP.</p>"},{"location":"workshops/clustering_a/#check-your-raw-data","title":"Check your raw data","text":"<p>Play your bag using the following command (or however you wish): <pre><code>ros2 bag play /mnt/c/bag/lexus3-2024-04-05-gyor.mcap -l\n</code></pre></p> <p>Info</p> <p>The <code>-l</code> option in the <code>play</code> command loops the mcap file.</p> <p>Success</p> <p>If everything works as expected you should see a bunch of topics in another terminal   Topics In another terminal issue the command: <p><pre><code>ros2 topic list\n</code></pre> You should see a similar list of topics:</p> <p><pre><code>/clock\n/events/read_split\n/lexus3/gps/duro/current_pose\n/lexus3/gps/duro/imu\n/lexus3/gps/duro/mag\n/lexus3/gps/duro/navsatfix\n/lexus3/gps/duro/status_flag\n/lexus3/gps/duro/status_string\n/lexus3/gps/duro/time_diff\n/lexus3/gps/duro/time_ref\n/lexus3/os_center/points\n/lexus3/os_left/points\n/lexus3/os_right/points\n/lexus3/zed2i/zed_node/left/image_rect_color/compressed\n/parameter_events\n/rosout\n/tf\n/tf_static   \n</code></pre> </p> <p>Also there must be at least one <code>sensor_msgs/msg/PointCloud2</code>, check with: <pre><code> ros2 topic type /lexus3/os_center/points\n</code></pre> Result: <pre><code>sensor_msgs/msg/PointCloud2\n</code></pre></p>"},{"location":"workshops/clustering_a/#step-2-install-ros-2-packages","title":"<code>Step 2.</code> - Install <code>ROS 2</code> packages","text":"<p>Info</p> <p>If you don't have the <code>~/ros2_ws/</code> directory already, create it with the following command: <pre><code>mkdir -p ~/ros2_ws/src\n</code></pre> If you have your own workspace, make sure to update the paths accordingly in the following steps.</p>"},{"location":"workshops/clustering_a/#clone-patchworkpp-package","title":"Clone <code>patchworkpp</code> package","text":"<p><code>patchwork-plusplus-ros</code> is ROS 2 package of Patchwork++ (@ IROS'22), which provides fast and robust LIDAR ground segmentation. We recommend the JKK research fork which contains some improvements, alternatively you can use the original KAIST version. </p> <p><pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>git clone https://github.com/jkk-research/patchwork-plusplus-ros\n</code></pre> or <pre><code>git clone https://github.com/url-kaist/patchwork-plusplus-ros -b ROS2\n</code></pre></p>"},{"location":"workshops/clustering_a/#clone-lidar_cluster-package","title":"Clone <code>lidar_cluster</code> package","text":"<pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>git clone https://github.com/jkk-research/lidar_cluster_ros2\n</code></pre>"},{"location":"workshops/clustering_a/#build-the-packages","title":"Build the packages","text":"<pre><code>cd ~/ros2_ws\n</code></pre> <pre><code>colcon build --packages-select patchworkpp lidar_cluster --symlink-install\n</code></pre>"},{"location":"workshops/clustering_a/#step-3-run","title":"<code>Step 3.</code> - Run","text":""},{"location":"workshops/clustering_a/#what-to-expect","title":"What to expect","text":"<pre><code>graph TD;\n\n    p1[ /lexus3/os_center/points&lt;br/&gt;sensor_msgs::PointCloud2]:::white --&gt; patchwork([ /patchwork_node]):::light\n    patchwork --&gt; p\n    p[ /nonground&lt;br/&gt;sensor_msgs::PointCloud2]:::white --&gt; cluster([ /cluster_node]):::light\n    cluster --&gt; f1[ /clustered_points&lt;br/&gt;sensor_msgs::PointCloud2]:::white\n    cluster --&gt; f2[ /clustered_marker&lt;br/&gt;visualization_msgs::MarkerArray]:::white\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#15274\n    classDef dash fill:#ffffff,stroke:#152742,stroke-width:2px,color:#15274, stroke-dasharray: 5 5\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre>  Don't forget to source before ROS commands. <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre> <pre><code>ros2 bag play /mnt/c/bag/lexus3-2024-04-05-gyor.mcap -l\n</code></pre> <p><pre><code>ros2 launch patchworkpp demo.launch.py  cloud_topic:=/lexus3/os_center/points cloud_frame:=lexus3/os_center_a_laser_data_frame\n</code></pre> Use one of the following clustering algorithms:</p> <p><pre><code>ros2 launch lidar_cluster dbscan_spatial.launch.py\n</code></pre> DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a non-grid based clustering algorithm. On a modern 6-core CPU or better, you can expect a performance of at least 10 Hz. </p> <p><pre><code>ros2 launch lidar_cluster euclidean_spatial.launch.py\n</code></pre> Non-grid clustering based on euclidean distance.  On a modern 6-core CPU or better, you can expect a performance of at least 5 Hz. </p> <p><pre><code>ros2 launch lidar_cluster euclidean_grid.launch.py\n</code></pre> Voxel grid based clustering based on euclidean distance. On a modern 6-core CPU or better, you can expect a performance of at least 100 Hz. </p> <pre><code>ros2 launch lidar_cluster rviz02.launch.py\n</code></pre> <p>Success</p> <p>If everything works as expected you should see a similar rviz window.  </p>"},{"location":"workshops/control_sim/","title":"Control simulation","text":""},{"location":"workshops/control_sim/#ros-2-sim_wayp_plan_tools-package","title":"ROS 2 <code>sim_wayp_plan_tools</code> package","text":"<p>Gazebo Fortress ROS 2 simulation for the waypoint and planner tools.</p> <p></p>"},{"location":"workshops/control_sim/#requirements","title":"Requirements","text":"<ul> <li>ROS 2 Humble: docs.ros.org/en/humble/Installation.html</li> <li>Gazebo Fortress: gazebosim.org/docs/fortress/install_ubuntu, read more about integration: gazebosim.org/docs/fortress/ros2_integration</li> <li><code>ros-gz-bridge</code> install with a single command: <code>sudo apt install ros-humble-ros-gz-bridge</code></li> </ul>"},{"location":"workshops/control_sim/#packages-and-build","title":"Packages and build","text":"<p>It is assumed that the workspace is <code>~/ros2_ws/</code>.</p>"},{"location":"workshops/control_sim/#clone-the-packages","title":"Clone the packages","text":"<p>Clone wayp_plan_tools and sim_wayp_plan_tools packages.</p> <pre><code>cd ~/ros2_ws/src &amp;&amp; \\\ngit clone https://github.com/jkk-research/wayp_plan_tools &amp;&amp; \\\ngit clone https://github.com/jkk-research/sim_wayp_plan_tools\n</code></pre>"},{"location":"workshops/control_sim/#build-ros-2-packages","title":"Build ROS 2 packages","text":"<p><pre><code>cd ~/ros2_ws\n</code></pre> <pre><code>colcon build --packages-select wayp_plan_tools sim_wayp_plan_tools --symlink-install\n</code></pre></p>"},{"location":"workshops/control_sim/#usage-of-wayp_plan_tools-as-a-simulation","title":"Usage of <code>wayp_plan_tools</code> as a simulation","text":""},{"location":"workshops/control_sim/#1-start-the-simulation","title":"1. Start the simulation","text":"<pre><code>ign gazebo -v 4 -r ackermann_steering.sdf\n</code></pre>"},{"location":"workshops/control_sim/#2-start-the-gazebo-bridge","title":"2. Start the Gazebo bridge","text":"Open a neew terminal and don't forget to source before ROS commands. <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre> <pre><code>ros2 launch sim_wayp_plan_tools gazebo_bridge.launch.py\n</code></pre> <p>In the background this <code>launch</code> file starts nodes similar to:</p> <p><pre><code>ros2 run ros_gz_bridge parameter_bridge /world/ackermann_steering/pose/info@geometry_msgs/msg/PoseArray[ignition.msgs.Pose_V\nros2 run ros_gz_bridge parameter_bridge /model/vehicle_blue/cmd_vel@geometry_msgs/msg/Twist]ignition.msgs.Twist\nros2 run ros_gz_bridge parameter_bridge /model/vehicle_blue/odometry@nav_msgs/msg/Odometry[ignition.msgs.Odometry --ros-args -r /model/vehicle_blue/odometry:=/odom\n</code></pre> More about the bridge here: github.com/gazebosim/ros_gz/blob/ros2/ros_gz_bridge/README.md</p> <p>Also this <code>launch</code> creates <code>/tf</code> from the <code>PoseArray</code> with <code>pose_arr_to_tf</code>.</p>"},{"location":"workshops/control_sim/#optional-teleoperation-via-keyboard","title":"Optional: teleoperation via keyboard","text":"<pre><code>ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args -r /cmd_vel:=/model/vehicle_blue/cmd_vel\n</code></pre>"},{"location":"workshops/control_sim/#3-load-waypoints","title":"3. Load waypoints","text":"<p>Use you ROS 2 workspace as <code>file_dir</code>: <pre><code>ros2 run wayp_plan_tools waypoint_loader --ros-args -p file_name:=sim_waypoints1.csv -p file_dir:=$HOME/ros2_ws/src/sim_wayp_plan_tools/csv -r __ns:=/sim1\n</code></pre> Or simply with default parameters:</p> <pre><code>ros2 launch sim_wayp_plan_tools waypoint_loader.launch.py\n</code></pre>"},{"location":"workshops/control_sim/#4-waypoint-to-target","title":"4. Waypoint to target","text":"<p><pre><code>ros2 run wayp_plan_tools waypoint_to_target --ros-args -p lookahead_min:=2.5 -p lookahead_max:=4.5 -p mps_alpha:=1.5 -p mps_beta:=3.5 -p waypoint_topic:=waypointarray -p tf_frame_id:=base_link -p tf_child_frame_id:=map -r __ns:=/sim1\n</code></pre> Or simply with default parameters:</p> <pre><code>ros2 launch sim_wayp_plan_tools waypoint_to_target.launch.py\n</code></pre>"},{"location":"workshops/control_sim/#5-start-a-control","title":"5. Start a control","text":"<p>There are some options:</p> <ul> <li><code>single_goal_pursuit</code>: Pure pursuit (for vehicles / robots), a simple cross-track error method</li> <li><code>multiple_goal_pursuit</code>: Multiple goal pursuit for vehicles / robots an implementation of our paper</li> <li><code>stanley_control</code>: Stanley controller, a heading error + cross-track error method</li> <li><code>follow_the_carrot</code>: Follow-the-carrot, the simplest controller</li> </ul> <p></p> <p>This is a pure pursuit example:</p> <p><pre><code>ros2 run wayp_plan_tools single_goal_pursuit --ros-args -p cmd_topic:=/model/vehicle_blue/cmd_vel -p wheelbase:=1.0 -p waypoint_topic:=targetpoints -r __ns:=/sim1\n</code></pre> Or simply with default parameters:</p> <pre><code>ros2 launch sim_wayp_plan_tools single_goal_pursuit.launch.py\n</code></pre>"},{"location":"workshops/control_sim/#6-visualize-results-in-rviz2","title":"6. Visualize results in <code>RViz2</code>","text":"<pre><code>ros2 launch sim_wayp_plan_tools rviz1.launch.py\n</code></pre>"},{"location":"workshops/control_sim/#or-run-everything-with-a-single-command","title":"Or run everything with a single command","text":"<p>After <code>ign gazebo -v 4 -r ackermann_steering.sdf</code> (terminal 1) open a new terminal.</p>  Don't forget to source bashrc. <pre><code>source ~/.bashrc\n</code></pre> <p>Run this command (in terminal 2): </p> <pre><code>ros2 launch sim_wayp_plan_tools all_in_once.launch.py\n</code></pre>"},{"location":"workshops/control_sim/#evaluation","title":"Evaluation","text":"<pre><code>ros2 run rqt_reconfigure rqt_reconfigure\n</code></pre> <p>The following image shows some example runs:</p> <p></p>"},{"location":"workshops/control_sim/#troubleshoot","title":"Troubleshoot","text":"<p>Kill <code>ign gazebo server</code> if stuck:</p> <pre><code>ps aux | grep ign\n</code></pre> <pre><code>ab  12345 49.9  1.2 2412624 101608 ?      Sl   08:26  27:20 ign gazebo server\nab  12346  518  6.6 10583664 528352 ?     Sl   08:26 283:45 ign gazebo gui\nab  12347  0.0  0.0   9396  2400 pts/2    S+   09:21   0:00 grep --color=auto ign\n</code></pre> <p>Once you have identified the PID, use the kill command followed by the PID to terminate the process. For example:</p> <pre><code>kill 12345\n</code></pre>"},{"location":"workshops/f1tenth_real_a/","title":"<code>ROS 2</code> F1/10 hands-on workshop","text":"<ul> <li>A small overview of the <code>Bavarian-Hungarian Self-driving vehicles</code> workshop.</li> <li>Date: 2024.06.08. </li> <li>Place: Gy\u0151r, Hungary.</li> </ul>"},{"location":"workshops/f1tenth_real_a/#a-ros-2-package","title":"A <code>ROS 2</code> package","text":""},{"location":"workshops/f1tenth_real_a/#hands-on","title":"Hands-on","text":"<p><code>ROS 2</code> humble </p>"},{"location":"workshops/f1tenth_sim_a/","title":"<code>ROS 2</code> F1/10 Wheeltec Gazebo simulation workshop","text":"<p>The workshop is ROS 2 compatible  and </p>"},{"location":"workshops/f1tenth_sim_a/#video","title":"Video","text":"<p>Part 1 of the video series about the workshop is available on YouTube:</p> <p>Part 2 (TODO)</p>"},{"location":"workshops/f1tenth_sim_a/#requirements-high-level","title":"Requirements (high-level)","text":"<ol> <li>ROS 2 Humble: \ud83d\udfe0 see previous workshops or docs.ros.org/en/humble/Installation.html </li> <li>Gazebo Fortress: \u2705 current workshop gazebosim.org/docs/fortress/install_ubuntu</li> <li><code>ROS gz bridge</code>:  \u2705 current workshop, ROS integration. Install with a single command: <code>sudo apt install ros-humble-ros-gz-bridge</code>, gazebosim.org/docs/fortress/ros2_integration</li> <li>Build and run custom worlds and models  \u2705 current workshop (e.g. <code>F1/10</code> / <code>Wheeltec, Roboworks</code>) </li> </ol> Official F1/10 vehicle vs Wheeltec Roboworks Ackermann Rosbot mini vehicle"},{"location":"workshops/f1tenth_sim_a/#binary-installation-on-ubuntu","title":"Binary Installation on Ubuntu","text":"<p>Fortress binaries are provided for Ubuntu Bionic, Focal and Jammy. All of the Fortress binaries are hosted in the osrfoundation repository. To install all of them, the metapackage <code>ignition-fortress</code> can be installed. The following is based on gazebosim.org/docs/fortress/install_ubuntu.</p> <p>First install some necessary tools:</p> <p><pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install lsb-release wget gnupg\n</code></pre></p> <p>Then install Ignition Fortress:</p> <p><pre><code>sudo wget https://packages.osrfoundation.org/gazebo.gpg -O /usr/share/keyrings/pkgs-osrf-archive-keyring.gpg\n</code></pre> <pre><code>echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/pkgs-osrf-archive-keyring.gpg] http://packages.osrfoundation.org/gazebo/ubuntu-stable $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/gazebo-stable.list &gt; /dev/null\n</code></pre> <pre><code>sudo apt-get update\n</code></pre> <pre><code>sudo apt-get install ignition-fortress\n</code></pre></p> <p>All libraries should be ready to use and the <code>ign gazebo</code> app ready to be executed.</p>"},{"location":"workshops/f1tenth_sim_a/#gazebo-fortress-ros-2-integration","title":"Gazebo Fortress ROS 2 integration","text":"<p>Issue the following command:</p> HumbleJazzy <pre><code>sudo apt install ros-humble-ros-gz-bridge\n</code></pre> <pre><code>sudo apt install ros-$ROS_DISTRO-ros-gz-bridge\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#additional-settings-to-wsl2","title":"Additional settings to WSL2","text":"<p>Warning - WSL2</p> <p>There is an issue, which can be set even in <code>~/.bashrc</code>:</p> <pre><code>export LIBGL_ALWAYS_SOFTWARE=1\n</code></pre> <p>Set it in <code>~/.bashrc</code>: <pre><code>echo \"export LIBGL_ALWAYS_SOFTWARE=1\" &gt;&gt; ~/.bashrc\n</code></pre></p>  Don't forget to source bashrc. <pre><code>source ~/.bashrc\n</code></pre> <p>After new terminal or <code>source</code>:</p> <pre><code>echo $LIBGL_ALWAYS_SOFTWARE\n</code></pre> <p>should  print <code>1</code>. Alternatively </p> <p><pre><code>cat ~/.bashrc | grep LIBGL\n</code></pre> should print the line.</p>"},{"location":"workshops/f1tenth_sim_a/#optional-install-rviz-2d-overlay-plugin","title":"Optional: Install <code>rviz-2d-overlay</code> plugin","text":"<p>For better visualization (rviz top left corner yellow text), install the <code>rviz-2d-overlay</code> plugin:</p> HumbleJazzy <pre><code>sudo apt install ros-humble-rviz-2d-overlay*\n</code></pre> <pre><code>sudo apt install ros-$ROS_DISTRO-rviz-2d-overlay*\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#check-the-installation","title":"Check the installation","text":"<p>Success</p> <p>Now the <code>ign gazebo</code> should work and the <code>ros2</code> commands should be available.</p> <p></p> <p>Try at least one of the following commands:</p> <pre><code>ign gazebo\n</code></pre> <pre><code>ign gazebo -v 4 -r ackermann_steering.sdf\n</code></pre> <pre><code>ign gazebo shapes.sdf\n</code></pre> <p></p> <pre><code>ign param --versions\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#packages-and-build","title":"Packages and build","text":"<p>Detailed description of the packages and build process.</p> <p>It is assumed that the workspace is <code>~/ros2_ws/</code>.</p> <p>The <code>robotverseny_gazebo24</code> package contains the Gazebo 2.4 world and model files for the Wheeltec Roboworks robot, while the <code>megoldas_sim24</code> package contains simple controllers to drive the robot in the simulation.</p> <pre><code>cd ~/ros2_ws/src\n</code></pre> <pre><code>git clone https://github.com/robotverseny/robotverseny_gazebo24\n</code></pre> <pre><code>git clone https://github.com/robotverseny/megoldas_sim24\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#build","title":"Build","text":"<p>Build the following packages:</p> <ul> <li><code>robotverseny_application</code></li> <li><code>robotverseny_description</code></li> <li><code>robotverseny_bringup</code></li> <li><code>robotverseny_gazebo</code></li> <li><code>megoldas_sim24</code></li> </ul> <pre><code>cd ~/ros2_ws\n</code></pre> <pre><code>colcon build --symlink-install --packages-select robotverseny_application robotverseny_description robotverseny_bringup robotverseny_gazebo megoldas_sim24\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#run-the-simulation","title":"Run the simulation","text":"Don't forget to source before ROS commands. <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre> <pre><code>ros2 launch robotverseny_bringup roboworks.launch.py\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#run-the-controllers","title":"Run the controllers","text":"<p>After starting the simulation, we will need a controller to drive the robot. Two simple controllers are provided: <code>simple_pursuit.py</code> and <code>follow_the_gap.py</code>. An image of the simulation with follow_the_gap controller is shown below.</p> <p></p> <p>In a new terminal:</p>  Don't forget to source before ROS commands. <pre><code>source ~/ros2_ws/install/setup.bash\n</code></pre> <p><pre><code>ros2 launch megoldas_sim24 megoldas1.launch.py # start simple_pursuit\n</code></pre> <pre><code>ros2 run megoldas_sim24 simple_pursuit.py\n</code></pre> <pre><code>ros2 launch megoldas_sim24 megoldas2.launch.py # start follow_the_gap\n</code></pre> <pre><code>ros2 run megoldas_sim24 follow_the_gap.py\n</code></pre></p> <p>The results can be seen in the animation below:</p> <p></p>"},{"location":"workshops/f1tenth_sim_a/#useful-commands","title":"Useful commands","text":"<p>Publish command topic: <pre><code>ros2 topic pub --once /roboworks/cmd_vel geometry_msgs/msg/Twist \"{linear: {x: 2.5, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: -0.01}}\"\n</code></pre></p> <p>Teleop twist keyboard: <pre><code>ros2 run teleop_twist_keyboard teleop_twist_keyboard --ros-args -r /cmd_vel:=/roboworks/cmd_vel\n</code></pre></p> <p>Ignition info topic: <pre><code>ign topic -i --topic /model/roboworks/cmd_vel\n</code></pre> Ignition echo topic:</p> <pre><code>ign topic -et /model/roboworks/cmd_vel\n</code></pre> <p>Topics:</p> <pre><code>ros2 topic list\n</code></pre>  Here are the topics. <pre><code>/clicked_point\n/clock\n/goal_pose\n/initialpose\n/joint_states\n/parameter_events\n/robot_description\n/roboworks/cmd_vel\n/roboworks/odometry\n/roboworks/scan\n/rosout\n/tf\n/tf_static\n</code></pre>"},{"location":"workshops/f1tenth_sim_a/#transformations","title":"Transformations","text":"<p>The frame <code>/odom_combined</code> is practically the same as <code>/map</code>, there is a static <code>0,0,0</code> transform between them. The only dynamic transform is between <code>/odom_combined</code> and <code>/base_link</code>.</p> <pre><code>graph TD;\n\n    map([ map]):::light\n    odom_combined([ odom_combined]):::light\n    base_link([ base_link]):::light\n    chassis([ chassis]):::light\n    camera_link([ camera_link]):::light\n    imu_link([ imu_link]):::light\n    laser([ laser]):::light\n\n    odom_combined -.-&gt;|dynamic| base_link\n    base_link --&gt;|static| chassis\n    base_link --&gt;|static| camera_link\n    base_link --&gt;|static| imu_link\n    base_link --&gt;|static| laser\n    map ==&gt;|static - same| odom_combined\n\nclassDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \nclassDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\nclassDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\nclassDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre> <p>You can visualize the frames with:</p> <pre><code>ros2 run rqt_tf_tree rqt_tf_tree\n</code></pre> <p>Danger</p> <p>There might be even more frames, but we are not using them.</p>"},{"location":"workshops/f1tenth_sim_a/#additional-resources","title":"Additional resources","text":"<ul> <li>github.com/robotverseny</li> <li>github.com/robotverseny/robotverseny_gazebo24</li> <li>github.com/robotverseny/megoldas_sim24</li> <li>robotverseny.github.io</li> <li>sze-info.github.io/ajr/szimulacio/f1tenth_sim_a</li> </ul>"},{"location":"workshops/ros2_a/","title":"ROS 2 install","text":""},{"location":"workshops/ros2_a/#ros-2-install-hands-on-workshop","title":"<code>ROS 2</code> install hands-on workshop","text":"<p>This document will guide you through the installation process of <code>ROS 2</code>.</p> <p>This document also served as small overview of the <code>Bavarian-Hungarian Self-driving vehicles</code> workshop. Date: 2024.06.08, place: Gy\u0151r, Hungary.</p>"},{"location":"workshops/ros2_a/#install-ros-2-video","title":"Install <code>ROS 2</code> video","text":""},{"location":"workshops/ros2_a/#windows-wsl2","title":"Windows WSL2","text":"<p>Windows Subsystem for Linux is a compatibility layer for native running of Linux-based elements on Windows 10 or Windows 11-based systems. You should choose to use WSL if you do not want to install native Ubuntu (e.g. 18.04 / 22.04) on your computers.</p> <p>Steps to install:</p> <ul> <li>Run as administrator, open a PowerShell window.</li> <li>Copy the command below. You hereby authorize the use of WSL. <pre><code>Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux\n</code></pre></li> <li>Restart the computer by entering the letter <code>Y</code>. (optional)</li> <li>Open the Microsoft Store and search for Windows Subsystem for Linux Preview. Install it.</li> <li>Also search for Ubuntu 22.04 in the Microsoft Store and install or PowerShell (Admin): <pre><code>wsl --install -d Ubuntu-22.04\n</code></pre></li> <li>For easier handling, it is worth installing the Windows Terminal program as well. Also search for Windows Terminal in the Microsoft Store and install it.</li> <li>Start the Windows Terminal program and open the settings with the Ctrl+, (Control and comma) key combination. Select Ubuntu 22.04 from the drop-down list of the Default Profile setting line.</li> <li>Restart the Windows Terminal. When starting for the first time, enter a user name and password of your choice.</li> <li>We recommend the VS Code editor to develop the solution. Install from: code.visualstudio.com/download</li> <li>Finally, install the VS Code Remote Development add-on to be available using WSL: marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack</li> </ul>"},{"location":"workshops/ros2_a/#installation-after-you-have-ubuntu","title":"Installation (after you have Ubuntu)","text":"<p>ROS 2</p> <p>ROS 1 versions are only discussed for historical reasons, ROS 2 is recommended for current developments.</p> <p><code>ROS 1</code> is basically supported on Linux systems, although there have been attempts for other operating systems as well. On the other hand, <code>ROS 2</code> already supports running on native Windows, Mac OS or other Real-Time operating systems. So basically there are four options:</p> <ol> <li>Dual boot, native Linux installed alongside Windows (mostly Ubuntu) \u2705 description</li> <li>Windows WSL2, lightweight Linux virtual machine \u2705 description</li> <li>Virtual machine for Windows \ud83d\udfe0</li> <li>Windows build \ud83d\udfe0</li> </ol> <p>We recommend the first two of these 4 options, but of course the others are not prohibited either. Dual boot provides an insight into the Linux world, which is useful knowledge for an engineer today. Care must be taken during installation, as a wrong setting causes data loss, so a backup is also recommended. WSL (Windows Subsystem for Linux) is a lightweight compatibility layer for running Linux-based components on Windows 10 or Windows 11-based systems. As you can see in the following figure, the Linux kernel can access the hardware elements (CPU, memory, GPU) just as easily as the Windows kernel. Compared to this, the virtual machine (option 3) is a much slower solution that uses more abstraction layers, and is recommended for those who either have a very modern, fast machine or have already installed such systems. The native Windows build (option 4) is a given in principle, but since the majority of the documentation is available for Linux, it will mean a lot of extra work.</p> <p>Illustration of the first three options:</p> <p></p> <p>The following description applies to Ubuntu 22.04 Jammy. Note that other versions are also supported, their installation and descriptions are available here: docs.ros.org/en/humble/Installation/Alternatives.html</p> <p>The following description is based on docs.ros.org/en/humble/Installation.html.</p>"},{"location":"workshops/ros2_a/#set-language","title":"Set language","text":"<p>Note</p> <p>This step can usually be skipped</p> <p>Make sure you have a locale that supports UTF-8.</p> <pre><code>locale # Check for UTF-8\n\nsudo apt update &amp;&amp; sudo apt install locales\nsudo locale-gen en_US en_US.UTF-8\nsudo update-locale LC_ALL=en_US.UTF-8 LANG=en_US.UTF-8\nexport LANG=en_US.UTF-8\n\ncheck locale # settings\n</code></pre>"},{"location":"workshops/ros2_a/#set-resources","title":"Set resources","text":"<p>You need to add the ROS 2 apt repository to your system.</p> <p>First, make sure the Ubuntu Universe repository is enabled.</p> <pre><code>sudo apt install software-properties-common\nsudo add-apt-repository universe\n</code></pre> <p>Add ROS 2 GPG key with <code>apt</code>.</p> <pre><code>sudo apt update &amp;&amp; sudo apt install curl -y\nsudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key -o /usr/share/keyrings/ros-archive-keyring.gpg\n</code></pre> <p>Next comes the addition of the container to the source list.</p> <pre><code>echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release &amp;&amp; echo $UBUNTU_CODENAME) main\" | sudo tee /etc/apt/sources.list.d/ros2.list &gt; /dev/null\n</code></pre>"},{"location":"workshops/ros2_a/#install-ros-2-packages","title":"Install ROS 2 packages","text":"<p>Update:</p> <pre><code>sudo apt update\n</code></pre> <p>ROS 2 packages are often built on updated Ubuntu systems. It is always recommended to make sure your system is up-to-date before installing new packages. <pre><code>sudo apt upgrade\n</code></pre></p> <p>Desktop installation: Installation of ROS, RViz, demos, tutorials: <pre><code>sudo apt install ros-humble-desktop\n</code></pre></p> <p>Development tools, compilers, and other tools for building ROS packages: <pre><code>sudo apt install ros-dev-tools\n</code></pre></p>"},{"location":"workshops/ros2_a/#source","title":"Source","text":"<p>Set up your environment by sourcing the following file:</p> <pre><code>source /opt/ros/humble/setup.bash\n</code></pre> <p>Tip: this can also be done in the <code>.bashrc</code> file <code>echo \"source /opt/ros/humble/setup.bash\" &gt;&gt; ~/.bashrc</code>.</p>"},{"location":"workshops/ros2_a/#check-installation","title":"Check installation","text":"<p>We check the correctness of the installation with the <code>ros2 topic list</code> command.</p> <pre><code>$ ros2 topic list\n\n/parameter_events\n/rosout\n</code></pre> <p>If everything is fine, the above two topics should appear. Then you can learn how to use simple example nodes: docs.ros.org/en/humble/Tutorials/Beginner-CLI-Tools.html</p>"},{"location":"workshops/ros2_a/#recommended-settings-after-installation","title":"Recommended settings after installation","text":""},{"location":"workshops/ros2_a/#console-colors","title":"Console colors","text":"<p>By default, the console output is not colored, but it is advisable to set this with the <code>RCUTILS_COLORIZED_OUTPUT</code> environment variable (even written in <code>bashrc</code>). For example:</p> <pre><code>export RCUTILS_COLORIZED_OUTPUT=1\n</code></pre> <p></p> <p>Details: docs.ros.org/en/humble/Tutorials/Demos/Logging-and-logger-configuration.html#id14</p>"},{"location":"workshops/ros2_a/#colcon_cd","title":"<code>colcon_cd</code>","text":"<p>It is also advisable to set the <code>colcon_cd</code> command so that you can quickly change your working directory to the directory of a package. As an example, the command <code>colcon_cd some_ros_package</code> can quickly jump to the directory <code>~/ros2_ws/src/some_ros_package</code>.</p> <p>Details: docs.ros.org/en/humble/Tutorials/Beginner-Client-Libraries/Colcon-Tutorial.html#setup-colcon-cd</p>"},{"location":"workshops/ros2_a/#home-laboratory-room-installation","title":"Home / laboratory room installation","text":"<p>In the lab, we run the following <code>install_humble.sh</code> file (shell script) on each machine.</p> <p><pre><code>wget https://raw.githubusercontent.com/sze-info/arj/main/docs/telepites/install_humble.sh\n</code></pre> <pre><code>sudo chmod +x install_humble.sh\n</code></pre></p> <p>At home: <pre><code>./install_humble.sh\n</code></pre> In the laboratory room (campus): <pre><code>./install_humble.sh campus\n</code></pre></p>"},{"location":"workshops/ros2_a/#workspace-reset","title":"Workspace reset","text":"<p>If we want to delete the entire <code>ros2_ws</code>, then clone and build it again (~5 minutes), we can do it with the following single long command:</p> <pre><code>cd ~ ; rm ws_reset.sh; wget https://raw.githubusercontent.com/sze-info/arj/main/docs/telepites/ws_reset.sh; sudo chmod +x ws_reset.sh; ./ws_reset.sh\n</code></pre>"},{"location":"workshops/ros2_b/","title":"ROS 2 introduction","text":""},{"location":"workshops/ros2_b/#ros-2-intro","title":"<code>ROS 2</code>  intro","text":"<p><code>ROS 2</code>, the latest release of <code>ROS</code>, is a set of software libraries and tools (middleware) that help develop robot applications. By definition, middleware is software that connects software components. It is a layer that sits between the operating system and applications on both sides of a distributed computer network. <code>ROS 2</code> uses the permissive open source Apache 2.0 license.</p> <pre><code>graph TD;\n\n    u[/Users /]:::light &lt;--&gt; p[/Program/]:::light &lt;--&gt; m\n    m[/Middleware /ROS 2/ /]:::red &lt;--&gt; o[/Operating system/]:::light\n    o &lt;--&gt; h[/Hardware/]:::light\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre> <p><code>ROS</code> has undergone incremental updates since its release in 2007, so there have been no fundamental changes, but major improvements have been made continuously. In 2017, the robotics community realized that the original 2007 concept has fundamental limitations that unfortunately cannot be improved in such an incremental way. Thus, in the end, Noetic Ninjemys (supported until 2025) is the last release of <code>ROS 1</code>, instead <code>ROS 2</code> started to be developed in parallel. This also means that it is more difficult to port the previous source codes to the new version, in return we can get a lot of new features, improvements, and support for the robots and vehicles to be developed.</p> <p>As a result of the above, <code>ROS 2</code> moved from the world of academic research to industrial use. It is interesting that NASA's VIPER lunar rover also runs <code>ROS 2</code>. It is also used by automotive giants such as Bosch, BMW and Volvo. Many other examples could be given from robotics companies. Links: www.nasa.gov/viper/lunar-operations, rosindustrial.org/ric/current-members, www.bosch.com/stories/bringing-robotics-middleware-onto-tiny-microcontrollers. ROS users in the world: metrorobots.com/rosmap.html.</p> <p></p> <p>K\u00e9p forr\u00e1sa: Robot Operating System 2: Design, Architecture, and Uses In The Wild: Steve Macenski et al.</p>"},{"location":"workshops/ros2_b/#why-should-i-use-a-framework-for-my-robotics-project","title":"Why should I use a framework for my robotics project?","text":"<p>For our first robotics project, we can choose the path of creating our own solution without a framework. Obviously, this also has advantages (learning, running speed, etc.). But soon we will need an algorithm that has been implemented by others, but is not compatible with the original idea. Here it is advisable to consider using a framework (e.g. <code>ROS 2</code>). Note that <code>ROS 2</code> is not the only option, there are many similar, smaller frameworks: Player, YARP,  Orocos, CARMEN, Orca, MOOS, and Microsoft Robotics Studio. Obviously, all of them have advantages, but in this case, due to the support, we narrow it down to <code>ROS 2</code>.</p> <p></p> <p>Image source: ros.org/blog/ecosystem</p> <ul> <li>Plumbing: ROS basically provides a messaging system, often called \"middleware\" or \"plumbing\". Communication is one of the first needs that arise when implementing a new robotics application or any software system that is connected to hardware. The built-in and well-tested messaging system of ROS can save time, since it handles the details of communication between decentralized nodes, it does not need to be implemented separately. It is even possible to directly access memory on a machine using intra-process communication.</li> <li>Tools: Developing effective applications requires good development tools. ROS has such tools including: debugging (<code>rqt_console</code>), visualization (<code>Rviz2</code>, <code>Foxglove Studio</code>), plotting (<code>rqt_plot</code>, <code>Foxglove Studio</code>), logging (<code>mcap</code>) and replay.</li> <li>Capabilities: Whether it's a GPS device driver, a gait and balance controller for a quadruped robot, or a mapping system for a mobile robot, ROS has solutions to the problem. From drivers to algorithms to user interfaces, ROS provides the building blocks that allow you to focus on your application.</li> <li>Community: Behind ROS is a large, global and diverse community. From students and hobbyists to multinational companies and government agencies, all segments of people and organizations operate the <code>ROS 2</code> project. This is also important because a lot of questions will arise during development. Most of these have already been answered by the community, and they are usually happy to answer new questions.</li> </ul> <p>The following figure shows the nodes (programs) and topics (~communication) of a simple line-following robot:</p> <pre><code>graph TD;\n\n    camd([/cam_driver]):::red --&gt; im1[ /image1&lt;br/&gt;sensor_msgs/Image]:::light\n    im1 --&gt; li1([ /line_detect_node]):::red\n    im1 --&gt; st1([ /stop_detect_node]):::red\n    li1 --&gt; li2[ /line&lt;br/&gt;example_msgs/Line]:::light\n    st1 --&gt; st2[ /stop&lt;br/&gt;example_msgs/Stop]:::light\n    li2 --&gt; nav([ /line_detect_node]):::red\n    st2 --&gt; nav\n    nav --&gt; cmd[ /cmd_vel&lt;br/&gt;geometry_msgs/Twist]:::light\n    cmd --&gt; control([ /robot_control]):::red\n    n1([ /node]):::white -- publishes --&gt; t[ /topic&lt;br/&gt;msg_type]:::white\n    t -- subscribes --&gt; n2([ /node]):::white\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre> <p>Source: Bestmann, Marc &amp; Fakult\u00e4t, Min &amp; Zhang, Jianwei &amp; Hendrich, N.. (2017). Towards Using ROS in the RoboCup Humanoid Soccer League. Masterthesis</p> <p>Let's look at another example that creates maps from speed data, IMU, and distance data.</p> <pre><code>graph LR;\n\n    odom[ /odom&lt;br/&gt;nav_msgs/Odometry]:::light --&gt; slam([ /slam_node]):::red\n    speed[ /speed&lt;br/&gt;geometry_msgs/Twist]:::light --&gt; slam\n    imu[ /imu&lt;br/&gt;sensor_msgs/Imu]:::light --&gt; slam\n    scan[ /scan&lt;br/&gt;sensor_msgs/PointCloud2]:::light --&gt; slam\n    n1([ /node]):::white -- publishes --&gt; t[ /topic&lt;br/&gt;msg_type]:::white\n    slam --&gt; pose[ /global_pose&lt;br/&gt;geometry_msgs/Pose]:::light\n    slam --&gt; map_g[ /map_grid&lt;br/&gt;nav_msgs/OccupancyGrid]:::light\n    slam --&gt; map_p[ /map_points&lt;br/&gt;sensor_msgs/PointCloud2]:::light\n    t -- subscribes --&gt; n2([ /node]):::white\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre>"},{"location":"workshops/ros2_b/#ros-2-directory-structure","title":"<code>ROS 2</code> directory structure","text":"<pre><code>~/ros2_ws$ ls\n\nbuild  install  log  src\n</code></pre> <pre><code>graph TD;\n\n    W1{{ Workspace&lt;/br&gt;pl. ros2_ws }}:::light --&gt; S1{{ Source space&lt;/br&gt;src }}:::white\n    W1 --&gt; B1{{ Build space&lt;/br&gt;build }}:::white\n    W1 --&gt; I1{{ Install space&lt;/br&gt;install }}:::white\n    W1 --&gt; L1{{ Log space&lt;/br&gt;log }}:::white\n    S1 --&gt; P1{{ package1 }}:::white\n    S1 --&gt; P2{{ package2 }}:::white\n    S1 --&gt; P3{{ bundle_packages }}:::white\n    P1 --&gt; LA1{{ launch }}:::white\n    P1 --&gt; SR1{{ src }}:::white\n    P2 --&gt; LA2{{ launch }}:::white\n    P2 --&gt; SR2{{ src }}:::white\n\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre> <pre><code>graph TD;\n\n    W2{{ other_ws }}:::light --&gt; S2{{ src }}:::white\n    W2 --&gt; B2{{ build }}:::white\n    W2 --&gt; I2{{ install }}:::white\n    W2 --&gt; L2{{ log }}:::white\n\n    classDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \n    classDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\n    classDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\n    classDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff</code></pre> <pre><code>~/ros2_ws/\n\u251c\u2500\u2500build  \n\u251c\u2500\u2500install  \n\u251c\u2500\u2500log\n\u2514\u2500\u2500src/\n    \u251c\u2500\u2500 bundle_packages \n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cone_detection_lidar\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 launch\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 src\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 my_vehicle_bringup\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 launch\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 other bundle package1\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 other bundle package2\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 img\n    \u2514\u2500\u2500 wayp_plan_tools\n        \u251c\u2500\u2500 csv\n        \u251c\u2500\u2500 launch\n        \u2514\u2500\u2500 src\n</code></pre>"},{"location":"workshops/ros2_b/#differences-between-ros-1-and-ros-2","title":"Differences between <code>ROS 1</code> and <code>ROS 2</code>","text":"<ul> <li>Changes in Middleware <code>ROS 1</code> uses Master-Slave architecture and XML-RPC middleware. <code>ROS 2</code>, on the other hand, uses Data Distribution Service (DDS), which provides higher efficiency and reliability, low latency and scalability, as well as configurable quality of service (QoS) parameters. Among other things, you don't have to start `roscore' this way. XML-RPC is better for simple remote procedure calls, while the added complexity of DDS allows it to better support real-time systems.</li> <li> <p>Changes in ROS API <code>ROS 1</code> has two separate libraries: <code>roscpp</code> for C++ and <code>rospy</code> for Python. They are not exactly identical to each other in terms of functions. In contrast, <code>ROS 2</code> has a base library written in C - <code>rcl</code> (ROS client library) - on which libraries are built. This ensures that core functionality is available sooner in various APIs. This is one of the main reasons why <code>ROS 2</code> is able to provide more language support besides the previous Python and C++: for example rclada Ada, rclcpp C++, rclgo Go, rclpy Python, rcljava Java, rclnodejs Node.js, rclobjc  Objective C (iOS), rclc C, ros2_rust Rust, ros2_dotnet .NET, ros2cs ros2_dotnet alternative in C#.</p> </li> <li> <p>Changes in data format <code>ROS 2</code> uses the <code>MCAP</code> format, which is not a dedicated ROS proprietary format, but an open source container file format for multimodal log data. It supports time-stamped pre-queued data and is ideal for use in pub/sub or robotics applications. More: mcap.dev</p> </li> </ul>"},{"location":"workshops/ros2_b/#a-couple-of-useful-updates","title":"A couple of useful updates","text":"<ul> <li>Real-time processing  The summary of the above functions, as well as the use of DDS, make `ROS 2' well suited for real-time processing, especially when deterministic, low-latency communication is required.</li> <li>QoS: Quality of Service <code>ROS 2</code> allows you to configure data flow, which affects how data is sent and received. This includes message reliability, deadline, and priority settings that can ensure critical messages are delivered on time.</li> <li>Multi-threaded execution <code>ROS 2</code> supports truly parallel running of multiple nodes, so modern multi-core processors can be used much better than <code>ROS 1</code>.</li> </ul> <p> Source: husarnet.com/blog/ros2-docker</p>"},{"location":"workshops/ros2_b/#other-changes","title":"Other changes","text":"<ul> <li>Catkin is gone, replaced by Ament (Colcon) as build system. Overlays allow you to create a secondary workspace that does not affect the primary workspace - this is useful when you need to experiment with new packages, but without affecting the base configuration (called an \"underlay\").</li> <li><code>ROS 2</code> is not backwards compatible with <code>ROS 1</code>. Consequently, <code>ROS 1</code> packages will probably not work with <code>ROS 2</code> and would require reworking, and other software you used to use with <code>ROS 1</code> will no longer work.</li> <li><code>ROS 1</code> is mainly made for Ubuntu. <code>ROS 2</code> runs on MacOS, Windows, Ubuntu and other (even Real-Time) operating systems.</li> </ul>"},{"location":"workshops/ros2_b/#versions","title":"Versions","text":"<p>Percentage distribution of distros over time: metrics.ros.org/rosdistro_rosdistro.html</p> <p></p> <p><code>Humble Hawksbill</code> or <code>Humble</code> for short is a long term support (LTS) release, supported for 5 years (May 2022 to May 2027)</p> <p>Additional releases: docs.ros.org/en/humble/Releases.html</p>"},{"location":"workshops/ros2_b/#concepts","title":"Concepts","text":""},{"location":"workshops/ros2_b/#nodes","title":"Nodes","text":"<p>In the simplest terms, node means a ROS program (in other words, node). In the figure, it is marked with a round \ud83d\udd34 symbol. Their characteristics:</p> <ul> <li>\"Executables\" (c++ / py).</li> <li>Each node is a \"program\", a separated process. Small overhead can be achieved &gt;&gt; intra-process communication.</li> <li>ROS manages the threads (threading).</li> <li>There can be several threads inside a node.</li> <li>publish/subscribe topics.</li> <li>Several nodes can \"publish\" on a topic, and one node can \"listen\" to several topics.</li> </ul>"},{"location":"workshops/ros2_b/#topics","title":"Topics","text":"<p>Topics can be understood as a named \"port\" where nodes can communicate. In the figure, it is marked with a square symbol \ud83d\udfe6. Their characteristics:</p> <ul> <li>Responsible for information flow between nodes.</li> <li>The type of each topic is determined by the \"message\".</li> <li>\"many-to-many\" communication between nodes is allowed</li> </ul>"},{"location":"workshops/ros2_b/#messages","title":"Messages","text":"<ul> <li>The content and structure of a topic is determined by the message</li> <li>Application programming interface (API) for Nodes are defined in files with .msg extension</li> </ul>"},{"location":"workshops/ros2_b/#types-of-messages","title":"Types of messages","text":"<ul> <li>Primitive built-in types (std_msgs)</li> <li><code>bool</code>, <code>string</code>, <code>float32</code>, <code>int32</code>, <code>\u2026</code></li> <li>Higher-level built in types:</li> <li><code>geometry_msgs</code>: <code>Point</code>, <code>Polygon</code>, <code>Vector</code>, <code>Pose</code>, <code>PoseWithCovariance</code>, <code>\u2026</code></li> <li><code>nav_msgs</code>: <code>OccupancyGrid</code>, <code>Odometry</code>, <code>Path</code>, <code>\u2026</code></li> <li><code>sensors_msgs</code>: <code>Joy</code>, <code>Imu</code>, <code>NavSatFix</code>, <code>PointCloud</code>, <code>LaserScan</code>, \u2026</li> <li>T\u00e1mogatottak tov\u00e1bb\u00e1:</li> <li>Konstansok</li> <li>Felsorol\u00e1sok</li> <li>Be\u00e1gyazott defin\u00edci\u00f3k</li> </ul> <p>P\u00e9lda:</p> <pre><code>$ ros2 interface show geometry_msgs/msg/Point\nfloat64 x\nfloat64 y\nfloat64 z\n</code></pre> <pre><code>$ ros2 interface show std_msgs/msg/Header\nuint32 seq\ntime stamp\nstring frame_id\n</code></pre> <p>A <code>Header</code> \u00e9s a<code>Point</code> a t\u00edpusb\u00f3l \u00e9p\u00fcl fel a <code>PoseStamped</code> t\u00edpus strukt\u00far\u00e1ja:</p> <p><pre><code>$ ros2 interface show geometry_msgs/msg/PoseStamped\nstd_msgs/Header header\n  uint32 seq\n  time stamp\n  string frame_id\ngeometry_msgs/Pose pose\n  geometry_msgs/Point position # (1) \n    float64 x\n    float64 y\n    float64 z\n  geometry_msgs/Quaternion orientation # (2)\n    float64 x\n    float64 y\n    float64 z\n    float64 w\n</code></pre></p> <ol> <li> This should look familiar: <code>geometry_msgs/msg/Point</code> has already been discussed.</li> <li> <code>geometry_msgs/Quaternion</code> is a type that represents a rotation in 3D space.</li> </ol>"},{"location":"workshops/ros2_b/#publishing-subscribing","title":"Publishing / Subscribing","text":"<p>In the following, the node named <code>urban_road_filt</code> subscribes to <code>points</code> data, which is of type <code>PointCloud2</code>, and advertises messages of type <code>PointCloud2</code>, <code>MarkerArray</code>:</p> <pre><code>flowchart LR\n\nP[points]:::light --&gt;|sensor_msgs/PointCloud2| U([urban_road_filt]):::red\nU --&gt; |sensor_msgs/PointCloud2| A[curb]:::light\nU --&gt; |sensor_msgs/PointCloud2| B[road]:::light \nU --&gt; |sensor_msgs/PointCloud2| C[road_probably]:::light\nU --&gt; |sensor_msgs/PointCloud2| D[roi]:::light\nU --&gt; |visualization_msgs/MarkerArray| E[road_marker]:::light\n\nn1([ /node]):::white -- publishes&lt;/br&gt;topic_type --&gt; t[ /topic]:::white\nt -- subscribes&lt;/br&gt;topic_type --&gt; n2([ /node]):::white\n\nclassDef light fill:#34aec5,stroke:#152742,stroke-width:2px,color:#152742  \nclassDef dark fill:#152742,stroke:#34aec5,stroke-width:2px,color:#34aec5\nclassDef white fill:#ffffff,stroke:#152742,stroke-width:2px,color:#152742\nclassDef red fill:#ef4638,stroke:#152742,stroke-width:2px,color:#fff\n</code></pre>"},{"location":"workshops/ros2_b/#parameters","title":"Parameters","text":"<ul> <li>Not everything can be written using Publish/Subscribe</li> <li>Nodes may sometimes need parameterization</li> <li>Parameters can be:<ul> <li>Controller type</li> <li>Color thresholds;</li> <li>Camera resolution, etc</li> </ul> </li> </ul>"},{"location":"workshops/ros2_b/#launch-files","title":"Launch files","text":"<p>Batch execution of several nodes (ROS program). Keeping the <code>ROS 1</code> conventions, it can be an XML format file that can define almost all aspects/operations of ROS. More recently, however, these can also be <code>python</code> files, so we have much more freedom. Start Node, set / load parameters, map topic, pass command line arguments.</p>"},{"location":"workshops/ros2_b/#code-example","title":"Code example","text":"PythonC++ <pre><code>#  ros2 topic type /lexus3/gps/duro/current_pose\n#  geometry_msgs/msg/PoseStamped\n#  ros2 interface show geometry_msgs/msg/PoseStamped\n\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\n\n\nclass SimplePoseSub(Node):\n\n    def __init__(self):\n        super().__init__('simple_pose_sub')\n        self.sub1_ = self.create_subscription(PoseStamped, '/lexus3/gps/duro/current_pose', self.topic_callback, 10)\n\n\n\n    def topic_callback(self, msg):\n\n        self.get_logger().info('x: %.3f, y: %.3f', msg.pose.position.x, msg.pose.position.y)\n\n\n\n\ndef main(args=None):\n\n    rclpy.init(args=args)                   ## Initialize the ROS 2 client library\n    simple_pose_sub = SimplePoseSub()\n    rclpy.spin(simple_pose_sub)             ## Create a node and spin\n    simple_pose_sub.destroy_node()\n    rclpy.shutdown()                        ## Shutdown the ROS 2 client library\n\nif __name__ == '__main__':\n    main()\n</code></pre> <pre><code>// ros2 topic type /lexus3/gps/duro/current_pose\n// geometry_msgs/msg/PoseStamped\n// ros2 interface show geometry_msgs/msg/PoseStamped\n\n#include \"rclcpp/rclcpp.hpp\"\n#include &lt;memory&gt;\n#include \"geometry_msgs/msg/pose_stamped.hpp\"\nusing std::placeholders::_1;\n\nclass SimplePoseSub : public rclcpp::Node{\npublic:\n    SimplePoseSub() : Node(\"simple_pose_sub\")\n    {\n        sub1_ = this-&gt;create_subscription&lt;geometry_msgs::msg::PoseStamped&gt;(\"/lexus3/gps/duro/current_pose\", 10, std::bind(&amp;SimplePoseSub::topic_callback, this, _1));\n    }\n\nprivate:\n    void topic_callback(const geometry_msgs::msg::PoseStamped &amp;msg) const\n    {\n        RCLCPP_INFO(this-&gt;get_logger(), \"x: %.3f, y: %.3f\", msg.pose.position.x, msg.pose.position.y);\n    }\n    rclcpp::Subscription&lt;geometry_msgs::msg::PoseStamped&gt;::SharedPtr sub1_;\n    };\n\nint main(int argc, char *argv[])\n{\n    rclcpp::init(argc, argv);               // Initialize the ROS 2 client library\n    rclcpp::spin(\n        std::make_shared&lt;SimplePoseSub&gt;()); // Create a node and spin\n\n    rclcpp::shutdown();                     // Shutdown the ROS 2 client library\n    return 0;\n\n}\n</code></pre>"},{"location":"workshops/ros2_b/#sources","title":"Sources","text":"<ul> <li>docs.ros.org/en/humble</li> <li>ros.org/blog/ecosystem</li> <li>husarnet.com/blog/ros2-docker</li> <li>design.ros2.org/articles/intraprocess_communications.html</li> <li>Towards Using ROS in the RoboCup Humanoid Soccer League - Masterthesis</li> </ul>"},{"location":"workshops/wheeltec_real_a/","title":"<code>ROS 1</code> real robot workshop","text":"<ul> <li>A small overview of the <code>Bavarian-Hungarian Self-driving vehicles</code> workshop.</li> <li>Date: 2023.11.03. </li> <li>Place: Gy\u0151r, Hungary.</li> </ul>"},{"location":"workshops/wheeltec_real_a/#the-megoldas_zala23-ros-1-package","title":"The <code>megoldas_zala23</code> ROS 1 package","text":"<p>\ud83e\udd16 In the following a very simple wall/gap following approach will be presented and described. The origin of he code is based on the work of Suresh Babu (University of Virginia, license). Link to the original code: github.com/linklab-uva/f1tenth_gtc_tutorial.</p> <p>The name of the package is a comes from a hungarian expression (<code>megoldas</code>: solution / L\u00f6sung).</p>"},{"location":"workshops/wheeltec_real_a/#the-robot-used-in-the-competition","title":"The robot used in the competition","text":"<p>Wheeltec / Roboworks Rosbot mini Ackermann robot </p> <p>On-board computer - Nvidia Jetson Nano</p> <p>Sensors - Orbbec Depth Camera - LSN10 LIDAR</p>"},{"location":"workshops/wheeltec_real_a/#video","title":"Video","text":""},{"location":"workshops/wheeltec_real_a/#usage","title":"Usage","text":"<p>Prerequisites: - WiFi-enabled computer with Ubuntu 18.04 / 20.04 operating system and ROS Melodic / Noetic installation - Internet access (Ethernet cable or WiFi)</p> <ol> <li>Turn on the robot platform.</li> <li>Use the computer to connect to the WiFi network created by the robot. The name of the WiFi network is unique for each robot platform, the <code>#</code> at the end of the SSID changes according to the number of the robot platform: <pre><code>SSID: WHEELTEC_CAR_5.5_#\nPassword: dongguan\n</code></pre></li> <li>Use SSH to connect to the on-board computer of the robot platform with the following terminal command: <pre><code>ssh wheeltec@192.168.0.100\n</code></pre> A password will be required, the default password is <code>dongguan</code></li> </ol>"},{"location":"workshops/wheeltec_real_a/#internet-access-on-the-robot-platform","title":"Internet access on the robot platform","text":"<p>Software packages can be downloaded to the on-board computer of the robot platform, which requires internet access.</p> <ul> <li>Ethernet: connect the Ethernet cable to the Ethernet port of the on-board computer of the robot platform.</li> <li>WiFi: after issuing the <code>nmtui</code> terminal command, connect to the available WiFi network. <pre><code>nmtui\n</code></pre></li> </ul> <p></p>"},{"location":"workshops/wheeltec_real_a/#install-the-ros-1-package","title":"Install the <code>ROS 1</code> package","text":"<p>After installation, the functions of the robot platform can be accessed using ROS. The sample solution of the competition can also be deployed by ROS.</p> <p>Create a workspace and install the sample solution on the robot:</p> <pre><code>mkdir -p ~/workshop_ws/src\n</code></pre> <pre><code>cd ~/workshop_ws/\n</code></pre> <pre><code>catkin init\n</code></pre> <pre><code>cd ~/workshop_ws/src/\n</code></pre> <pre><code>git clone https://github.com/robotverseny/megoldas_zala23\n</code></pre> <pre><code>cd ~/workshop_ws/\n</code></pre> <pre><code>catkin build megoldas_zala23\n</code></pre> <pre><code>echo \"source /home/wheeltec/workshop_ws/devel/setup.bash\" &gt;&gt; ~/.bashrc\n</code></pre> <pre><code>source ~/.bashrc\n</code></pre> <p>Install <code>screen</code> <pre><code>sudo apt install mc screen\n</code></pre></p> <p>Install jks visualization rviz plugin: depending on ROS 1 version (melodic / noetic):</p> <pre><code>sudo apt install ros-melodic-jsk-rviz-plugins\n</code></pre> <pre><code>sudo apt install ros-noetic-jsk-rviz-plugins\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#usage_1","title":"Usage","text":""},{"location":"workshops/wheeltec_real_a/#start-solution-using-screen-recommended","title":"Start solution using screen (recommended)","text":"<p>The script <code>verseny_start.sh</code> sets the required environmental variables, starts the ROS nodes and finally after 2 minutes stops everything. Have a look at the code: verseny_start.sh </p> <pre><code>rosrun megoldas_zala23 verseny_start.sh\n</code></pre> <p>The <code>verseny_start.sh</code> shell script usually launches several virtual terminals, such as: <code>roscore</code>, <code>turn_on_wheeltec_robot</code>, <code>lsn10_lidar</code>, <code>megoldas1.launch</code>. All components of the solution can be stopped with the following command: <pre><code>rosrun megoldas_zala23 stop_all.sh\n</code></pre></p> <p>Further commands:</p> <ul> <li>list screen: <code>screen -ls</code></li> <li>restore screen: <code>screen -r roscore</code> / <code>screen -r turn_on_wheeltec_robot</code> / <code>screen -r megoldas1</code></li> <li>detach: <code>Ctrl-a</code> + <code>Ctrl-d</code></li> </ul>"},{"location":"workshops/wheeltec_real_a/#ros-connection","title":"ROS connection","text":"<p>The ROS topics advertised by the robot platform are also available on the computer connected to the platform, with the appropriate setting of the <code>ROS_MASTER_URI</code> variable: <pre><code>export ROS_MASTER_URI=http://192.168.0.100:11311\n</code></pre> After the appropriate setting of the variable, the topics can be listed and visualized using Rviz: <pre><code>rostopic list\n</code></pre> <pre><code>rosrun rviz rviz\n</code></pre></p>"},{"location":"workshops/wheeltec_real_a/#some-explanatory-animations","title":"Some explanatory animations","text":"<pre><code>roslaunch megoldas_zala23 rviz1.launch\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#start-solution-per-component","title":"Start solution per component","text":"<p>The solution can also be started per component, not just as a single shell script. This requires four terminal windows on the on-board computer of the robot platform and issuing the following commands per terminal:</p> <pre><code>roscore\n</code></pre> <pre><code>roslaunch turn_on_wheeltec_robot turn_on_wheeltec_robot.launch\n</code></pre> <pre><code>roslaunch lsn10 lsn10.launch\n</code></pre> <pre><code>roslaunch megoldas_zala23 megoldas1.launch\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#additional-information","title":"Additional information","text":""},{"location":"workshops/wheeltec_real_a/#workspaces","title":"Workspaces","text":"<pre><code>~/wheeltec_robot/src\n~/catkin_workspace/src\n~/workshop_ws/src/\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#topic-management","title":"Topic management","text":"<pre><code>rostopic hz /scan\nrostopic echo /scan -n1\nrostopic type /scan\n</code></pre> <pre><code>sensor_msgs/LaserScan\n</code></pre>"},{"location":"workshops/wheeltec_real_a/#robot-platform-language-settings","title":"Robot platform language settings","text":"<pre><code>sudo dpkg-reconfigure locales\n</code></pre> <p>reboot</p>"},{"location":"workshops/wheeltec_real_a/#rosbag-management","title":"Rosbag management","text":"<p><pre><code>cd ~/rosbags\nrosbag record -a -o test1\n</code></pre> <pre><code>rsync -avzh --progress wheeltec@192.168.0.100:/home/wheeltec/rosbags/ /mnt/c/bag/wheeltec/\nrosbag info test1_2023-03-30-12-37-22.bag\nrosbag play test1_2023-03-30-12-37-22.bag\n</code></pre></p> <p>You can even visualize rosbags in Foxglove studio:</p> <p></p> <p>Download rosbags</p> <ul> <li>Further explanation ipynotebook</li> <li>Competition homepage</li> <li>Foxglove studio</li> </ul>"}]}